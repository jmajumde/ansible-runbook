#### ssh keyscan
Once the vagrang vms are up and running, we can scan the ssh pubkeys from target vms and add the keys into the known_hosts file for mgmt vm. This is one way to avoid the ssh trusted known hosts confirmation prompt

> ssh-keyscan lb web1 web2 >> .ssh/known_hosts


####Ansible ad-hoc commands
[https://assets.sysadmincasts.com/e/g/43-ansible-multi-node-deployment-workflow.png]

> ansible all -m ping --ask-pass

Since, we have already added the custom pubkey of mgmt vagrant host to all the target host web1..webn, lb via vagrant config definition in vagrantfile we can directly ssh from mgmt host to all target host.
> ansible all -m ping

We can install package with ad-hoc commands. The apt module allows you to install packages, just like you would on any Ubuntu machine, using the apt-get command.

> ansible web1 -m apt -a "name=ntp state=installed" web1 --sudo

Deploying configuration changes, files etc using ad-hoc commands

```
ansible web1 -m copy -a "src=/home/vagrant/files/ntp.conf dest=/etc/ntp.conf mode=644 owner=root group=root" --sudo
ansible web1 -m service -a "name=ntp state=restarted"
```
Though ansible has a provision to execute ad-hoc commands but that not the actual usecase for using ansible to automate devops tasks. The playbooks are the way to automate devops task by ansible. These ad-hoc commands sometimes comes handy.

> ansible all -m shell -a "uptime"

> ansible all -m shell -a "uname -a"

#### Gathering gather_facts
We can run an ad-hoc version, like this, ansible web1 -m setup

> ansible web1 -m setup | less

Lets add the argument option, and pass in, filter=ansible_distribution. So, you might want to run this across a bunch of machines to gather some data about them too, say for example that you are trying to get an inventory, based off some fact.

> ansible web1 -m setup -a "filter=ansible_distribution"

> ansible web1 -m setup -a "filter=ansible_distribution*"

#### Template and var
Ansible uses default jinja2 templating syntax. Check ntp-template.yml playbook and the corresponsing template is under template/ntp.conf.j2

#### Configuration mgmt of load balancer and nginx
The site.yml playbook has complete code for the HAProxy load balancer deployment and install, config nginx web hosts. The web page url should be localhost port 8080 which redirects to 80 in lb vagrant vm.

We can check the haproxy load balancer statistic page
> http://localhost:8080/haproxy?stats

This is the statistics report generated by haproxy and keeps up to date traffic totals across the load balancer. To see some meaningful numbers in this load balancer stat page, we can use apache bench mark to hit the url localhost:8080 concurrently.  

```
# installed apache bench on my host machine
sudo apt-get install apache2-utils
ab -n 10000 -c 25 http://localhost:8080/
```

#### Roles, make the tasks moduler
The role-site.yml has three plays in it, common, web, and lb just like site.yml but  it is much sorter, there is no tasks sections, but rather this roles definition. Roles allow you to remove bulky configuration out of the playbook, and into a folder structure called a role. The entire goal of this, is to make things modular, so that you can share and reuse roles, along with not having playbooks that are thousands of lines long


#### Zero-downtime rolling updates/patching/deployment
[https://assets.sysadmincasts.com/e/g/47-ansible-serial-pre-post-task-orchestration.gif]

For each node that we update, we want to verify that nginx is installed, and it has the correct config file. These steps are not required for a deployment, but is makes sense to ensure our hosting environment is in a sane state, and that is conforms to our known good requirements. Then, we will remove the existing website, and deploy our new website, and finally restart nginx if needed.

So in a nutshell, below is what we need to achieve through playbooks.

  pre_tasks:
    - disable web node in haproxy

  tasks:
  - write updated nginx.conf config file (file)
  - clean existing website content
  - deploy website content (from github)
  - start nginx (service)

  post_tasks:
    - enable web node in haproxy

To disable the web node from haproxy mark the server DOWN for maintenance. In this mode, no more checks will be performed on the server until it leaves maintenance.
[http://cbonte.github.io/haproxy-dconv/1.9/management.html#9.3-disable%20server]

```
root@lb:~# echo "disable server ansibleweb/web1" | socat stdio /var/lib/haproxy/stats

root@lb:~# echo "disable server ansibleweb/web2" | socat stdio /var/lib/haproxy/stats

```
The stats page http://localhost:8080/haproxy?stats would show web1 and web2 are down and the request is served by only web3 and web4. Once the web nodes are brought up (changing disable to enable in above command) from maintenance mode then the stats page shows corresponding web node again is up and serving request
